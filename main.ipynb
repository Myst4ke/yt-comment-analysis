{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YT Comments analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from datetime import datetime, timedelta, time\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "from api import API_KEY\n",
    "\n",
    "\n",
    "channel_id = \"UCWeg2Pkate69NFdBeuRFTAw\" #Squeezie channel\n",
    "etoiles = 'UCABf02qOye7XYapcK1M45LQ'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "exemple_video = \"qCKyRhkhqoQ\"\n",
    "otp_recap = 'F7A8OCdmZ90'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class request\n",
    "Class to handle youtube request since youtube api doesn't provide a request object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    \"\"\" Class Request handling youtube request as an object \"\"\"\n",
    "    def __init__(self, requestType,part=None, id=None, chart=None, regionCode=None, maxResults=None, pageToken=None, videoId=None):\n",
    "        self.requestType = requestType\n",
    "        self.part = part\n",
    "        self.id = id\n",
    "        self.chart = chart\n",
    "        self.regionCode = regionCode\n",
    "        self.maxResults = maxResults\n",
    "        self.pageToken = pageToken\n",
    "        self.videoId = videoId\n",
    "        \n",
    "    def execute(self):\n",
    "        param = vars(self) # Fetch class attributes\n",
    "        param = {x:y for x,y in list(param.items())[1:] if y} # Delete requestType ([1:]) and None attributes\n",
    "        \n",
    "        request = self.requestType.list(**param)\n",
    "        return request.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorator\n",
    "Decorator to retry when youtube request fails (mostly due to timeout erros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_on_exception(max_attempts=5):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempts = 0\n",
    "            while attempts < max_attempts:\n",
    "                try:\n",
    "                    result = func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    attempts += 1\n",
    "                    if attempts == max_attempts:\n",
    "                        return pd.DataFrame()\n",
    "                        raise  # Relancer l'exception si le nombre maximal de tentatives est atteint\n",
    "                    else:\n",
    "                        print(f\"{attempts}: Une exception s'est produite : {e}\")\n",
    "                else:\n",
    "                    return result  # Retourner le résultat si aucune exception n'est levée\n",
    "                # time.sleep(0.5)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime convertions\n",
    "Functions to convert iso formated date found in youtube api responses in datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_toDatetime(iso_date:str):\n",
    "    \"\"\"Converts an ISO 8601 formatted date to a datetime object.\"\"\"\n",
    "    return datetime.strptime(iso_date[:-1], '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "def datetime_toISO(dt_obj:datetime):\n",
    "    \"\"\"Converts a datetime object to an ISO 8601 formatted date.\"\"\"\n",
    "    return dt_obj.isoformat()[:-7]  # remove microseconds\n",
    "\n",
    "def iso_toDelta(iso_duration:str):\n",
    "    \"\"\"Converts an ISO 8601 formatted duration to a timedelta object.\"\"\"\n",
    "    match = re.match(r'PT(\\d+D)*(\\d+H)*(\\d+M)*(\\d+S)', iso_duration)\n",
    "    days, hours, minutes, seconds = [int(x[:-1]) if x else 0 for x in match.groups()]\n",
    "    return timedelta(days=days,hours=hours, minutes=minutes, seconds=seconds)\n",
    "\n",
    "def delta_toISO(delta_obj:timedelta):\n",
    "    \"\"\"Converts a timedelta object to an ISO 8601 formatted duration.\"\"\"\n",
    "    hours = delta_obj.seconds // 3600\n",
    "    minutes = (delta_obj.seconds % 3600) // 60\n",
    "    seconds = delta_obj.seconds % 60\n",
    "    \n",
    "    daysStr = f\"{delta_obj.days}D\" if delta_obj.days != 0 else \"\"\n",
    "    hoursStr = f\"{hours}H\" if hours != 0 else \"\"\n",
    "    minutesStr = f\"{minutes}M\" if minutes != 0 else \"\"\n",
    "    secondsStr = f\"{seconds}S\" if seconds != 0 else \"\"\n",
    "    return f\"PT{daysStr}{hoursStr}{minutesStr}{secondsStr}\"\n",
    "\n",
    "# print(iso_toDelta('PT4D3H20M9S'))\n",
    "# print(delta_toISO(iso_toDelta('PT20M9S')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fectching functions\n",
    "Functions to fetch channels, comments ant top vids infos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_channel_data(channel_data: dict) -> pd.DataFrame:\n",
    "    \"\"\" Structure raw channel data \"\"\"\n",
    "    data = {\n",
    "        \"channel_name\": [channel_data.get('snippet', {}).get('title')],\n",
    "        \"channel_id\": [channel_data.get('id')],\n",
    "        \"country\": [channel_data.get('snippet', {}).get('country',\"\")],\n",
    "        **{k:[int(v)] for k,v in channel_data.get('statistics', {}).items() if k != \"hiddenSubscriberCount\"},\n",
    "        \"topics\": [[wikilink.split('/')[-1] for wikilink in channel_data.get('topicDetails', {}).get('topicCategories', [])]],\n",
    "    }\n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_data(youtube, channel_id:str) -> dict[str|dict]:\n",
    "    \"\"\" Request (by id) for most important channel stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.channels(),\n",
    "        part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    rawData = response.get('items', [])[0]\n",
    "    return format_channel_data(rawData)\n",
    "    \n",
    "\n",
    "get_channel_data(youtube, etoiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_video_data(video_data: dict) -> pd.DataFrame:\n",
    "    \"\"\" Structure raw video data \"\"\"\n",
    "    data = {\n",
    "            \"title\": [video_data.get('snippet', {}).get('title')],\n",
    "            \"id\": [video_data.get('id')],\n",
    "            \"publishedAt\": [video_data.get('snippet', {}).get('publishedAt')],\n",
    "            \"duration\" : [video_data.get('contentDetails').get('duration')],\n",
    "            \"ViewCount\" : [int(video_data.get('statistics', {}).get('viewCount', 0))],\n",
    "            \"likeCount\" : [int(video_data.get('statistics', {}).get('likeCount', 0))],\n",
    "            \"commentCount\" : [int(video_data.get('statistics', {}).get('commentCount', 0))],  \n",
    "            \"tags\" : [video_data.get('snippet', {}).get('tags')]\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(youtube, video_Id:str) -> dict[str|dict]:\n",
    "    \"\"\" Request (by id) for most important video stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.videos(),\n",
    "        part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "        id=video_Id,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    rawData = response.get('items', [])[0]\n",
    "    return format_video_data(rawData)\n",
    "\n",
    "get_video_data(youtube, exemple_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Most_Popular_Video(youtube, region:str) -> pd.DataFrame:\n",
    "    \"\"\" Request for most populars videos stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.videos(),\n",
    "        part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "        chart=\"mostPopular\",\n",
    "        regionCode=region,\n",
    "        maxResults=100,\n",
    "        pageToken=''\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    pages = [response]\n",
    "    while response.get('nextPageToken'):\n",
    "        request.pageToken = response.get('nextPageToken')\n",
    "        response = request.execute()\n",
    "        pages.append(response)\n",
    "    \n",
    "    top_videos = pd.concat([format_video_data(videos) for page in pages for videos in page.get('items')]).reset_index(drop=True)\n",
    "    top_videos['topID'] = top_videos.index + 1\n",
    "    top_videos['region'] = region\n",
    "    return top_videos\n",
    "\n",
    "get_Most_Popular_Video(youtube, 'FR')\n",
    "# df.sort_values(by=['fetchedDate'], ascending=False, inplace=True, kind='quicksort', ignore_index=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_comment_data(comment:dict) -> pd.DataFrame:\n",
    "    \"\"\" Structure raw comment data \"\"\"\n",
    "    data = {\n",
    "        \"id\": [comment.get('id')],\n",
    "        \"comment\": [comment.get('snippet', {}).get('textOriginal')],\n",
    "        # \"viewerRating\": comment.get('snippet', {}).get('viewerRating'),\n",
    "        \"likeCount\": [int(comment.get('snippet', {}).get('likeCount'))],\n",
    "        \"publishedAt\": [comment.get('snippet', {}).get('publishedAt')],\n",
    "        \"updatedAt\": [comment.get('snippet', {}).get('updatedAt')]\n",
    "        }\n",
    "    \n",
    "    return data\n",
    "\n",
    "def format_threadedComment_data(comment:dict) -> pd.DataFrame:\n",
    "    \"\"\" Structure raw threaded comment data \"\"\"\n",
    "    data = {\n",
    "        **format_comment_data(comment.get('snippet', {}).get('topLevelComment')),\n",
    "        \"totalReplyCount\": [int(comment.get('snippet', {}).get('totalReplyCount'))],\n",
    "        # \"replies\": [format_comment_data(com) for com in comment.get('replies', {}).get('comments', [])]\n",
    "        }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_commentPage(page:list[dict]) -> pd.DataFrame:\n",
    "    data = {}\n",
    "    for comment in page:\n",
    "        topLevelComment = comment.get('snippet', {}).get('topLevelComment')\n",
    "        data.setdefault(\"id\", []).append(topLevelComment.get('id')) \n",
    "        data.setdefault(\"comment\", []).append(topLevelComment.get('snippet', {}).get('textOriginal'))\n",
    "        data.setdefault(\"likeCount\", []).append(int(topLevelComment.get('snippet', {}).get('likeCount')))\n",
    "        data.setdefault(\"publishedAt\", []).append(topLevelComment.get('snippet', {}).get('publishedAt'))\n",
    "        data.setdefault(\"updatedAt\", []).append(topLevelComment.get('snippet', {}).get('updatedAt'))\n",
    "        data.setdefault(\"totalReplyCount\", []).append(int(comment.get('snippet', {}).get('totalReplyCount')))\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(youtube,comment_id:str) -> dict[str|dict]:\n",
    "    \"\"\" Request (by id) for most important comment stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.comments(),\n",
    "        part=\"snippet,id\",\n",
    "        id=comment_id,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    rawData = response.get('items')[0]\n",
    "    return pd.DataFrame(format_comment_data(rawData))\n",
    "\n",
    "get_comment(youtube, 'UgwUQR2JJFJSkihWLhx4AaABAg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry_on_exception(max_attempts=3)\n",
    "def get_video_commentThreads(youtube, videoID:str, maxComments:int) -> dict[str|list]:\n",
    "    \"\"\" Request (by id) for all comments of a videos \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.commentThreads(),\n",
    "        part=\"snippet,id,replies\",\n",
    "        videoId=videoID,\n",
    "        maxResults=100\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    maxComments -= response.get('pageInfo', {}).get('totalResults')\n",
    "    comments = format_commentPage(response.get('items',{}))\n",
    "    while response.get('nextPageToken'):\n",
    "        request.pageToken = response.get('nextPageToken')\n",
    "        response = request.execute()\n",
    "        comments = pd.concat([comments, format_commentPage(response.get('items',{}))], ignore_index=True)\n",
    "        if (maxComments:= maxComments - response.get('pageInfo', {}).get('totalResults')) <= 0:\n",
    "            break\n",
    "        \n",
    "    comments['videoID'] = videoID\n",
    "    comments['fetchedDate'] = datetime.today()\n",
    "    return comments\n",
    "\n",
    "get_video_commentThreads(youtube, 'FkXhKu80CWU', 1000)\n",
    "# get_video_commentThreads(youtube, 'FkXhKu80CWU', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Top Videos\n",
    "The goal is to fetch the top 200 videos everyday and to get their comments a week after publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "def push_top_vids(topvidsFile:str, regions:list[str], minElapsedTime:int)-> None:\n",
    "    \"\"\" Fetch top 200 vids per region (per <minElapsedTime>) and push in json <topvidsFile> \"\"\"\n",
    "    today = datetime.today()\n",
    "    try:\n",
    "        df = pd.read_csv(topvidsFile)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    if 'fetchedDate' in df:\n",
    "        df['fetchedDate'] = pd.to_datetime(df['fetchedDate'], format='ISO8601')\n",
    "        lastUpdate = df.sort_values(by=['fetchedDate'], ascending=False, ignore_index=True).loc[0]['fetchedDate']\n",
    "        delta = today - lastUpdate\n",
    "        if delta.total_seconds() // 3600 < minElapsedTime:\n",
    "            raise Exception(f'The fetch request has be done too soon. Next request available in {24-(delta.total_seconds() // 3600)}h. Last update done : {lastUpdate}')\n",
    "\n",
    "    # Fetching\n",
    "    for reg in regions:\n",
    "        top200 = get_Most_Popular_Video(youtube, reg)\n",
    "        top200['fetchedDate'] = today\n",
    "        top200['fetchedComments'] = False\n",
    "        df = pd.concat([df, top200])\n",
    "        \n",
    "    df.to_csv('db/dailyTop200.csv', index=False) \n",
    "    \n",
    "def push_top_vids_with_db(regions: list[str], minElapsedTime: int) -> None:\n",
    "    \"\"\" Fetch top 200 vids per region (per <minElapsedTime>) and push into MongoDB collection \"\"\"\n",
    "    cluster = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = cluster[\"ProjectBigData\"]\n",
    "    today = datetime.today()\n",
    "    collection = db['topVideo']\n",
    "\n",
    "    # Récupérer la date de la dernière mise à jour dans la base de données MongoDB\n",
    "    last_record = collection.find_one(sort=[('fetchedDate', pymongo.DESCENDING)])\n",
    "    if last_record:\n",
    "        lastUpdate = datetime.strptime(last_record['fetchedDate'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "        print(lastUpdate)\n",
    "        delta = today - lastUpdate\n",
    "        if delta.total_seconds() // 3600 < minElapsedTime:\n",
    "            raise Exception(f'The fetch request has been done too soon. Next request available in {24 - (delta.total_seconds() // 3600)}h. Last update done: {lastUpdate}')\n",
    "\n",
    "    # Fetching\n",
    "    new_data = []  # Stocker les données récupérées pour une insertion ultérieure dans la base de données\n",
    "    for reg in regions:\n",
    "        top200 = get_Most_Popular_Video(youtube, reg)\n",
    "        top200['fetchedDate'] = today\n",
    "        top200['fetchedComments'] = False\n",
    "        new_data.extend(top200.to_dict(orient='records'))  # Étendre la liste des nouvelles données avec les données actuelles\n",
    "        \n",
    "    # Insérer toutes les nouvelles données dans la base de données MongoDB\n",
    "    if new_data:\n",
    "        collection.insert_many(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = ['FR', 'US']\n",
    "topvids = 'db/dailyTop200.csv'\n",
    "commentsQueue=\"db/comments.csv\"\n",
    "minElapsedTime = 24 # Hours\n",
    "minElapsedCommentsTime = 17 # days\n",
    "\n",
    "push_top_vids_with_db( REGION, minElapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timezone\n",
    "\n",
    "\n",
    "def fetch_topVids_comments(topvidsFile:str, commentsFile:str,minElapsedCommentsTime:int, maxComments:int = 1000) -> None:\n",
    "    today = datetime.today()\n",
    "    try:\n",
    "        df = pd.read_csv(topvidsFile)\n",
    "        comments = pd.read_csv(commentsFile)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        df = pd.DataFrame()\n",
    "        comments = pd.DataFrame()\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt'], format='ISO8601')\n",
    "    \n",
    "    print(df.dtypes)\n",
    "    print(df.shape)\n",
    "    \n",
    "    uniqueID = df.drop_duplicates(['id'])\n",
    "    uniqueID = uniqueID[uniqueID['fetchedComments'] == False]\n",
    "    id_list = list(uniqueID['id'])\n",
    "    \n",
    "    for id in id_list:\n",
    "        if (pd.to_datetime(today) - uniqueID.loc[uniqueID['id'] == id, 'publishedAt'].values[0]).days >= minElapsedCommentsTime:\n",
    "            print(id)\n",
    "            comments = pd.concat([comments, get_video_commentThreads(youtube, id, maxComments)])\n",
    "            df.loc[df['id'] == id, 'fetchedComments'] = True\n",
    "            \n",
    "    df.to_csv(topvidsFile, index=False)\n",
    "    comments.to_csv(commentsFile, index=False)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pytz\n",
    "\n",
    "\n",
    "def fetch_topVids_comments_with_bd(minElapsedCommentsTime:int, maxComments:int = 1000) -> None:\n",
    "    \n",
    "    cluster = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = cluster[\"ProjectBigData\"]\n",
    "\n",
    "    # Collection \"topVideo\"\n",
    "    collection = db['topVideo']\n",
    "        # Requête MongoDB pour récupérer les IDs des vidéos avec fetchedComments à false\n",
    "    pipeline = [\n",
    "    {\"$match\": {\"fetchedComments\": False}},  # Filtrer les vidéos avec fetchedComments égal à false\n",
    "    {\"$group\": {\"_id\": \"$id\", \"date\": {\"$first\": \"$publishedAt\"}}},  # Regrouper par l'ID de la vidéo et récupérer la première date\n",
    "    {\"$project\": {\"_id\": 0, \"id\": \"$_id\", \"date\": 1}}  # Projection pour renvoyer seulement l'ID et la date\n",
    "    ]\n",
    "\n",
    "    # Exécution de la requête et récupération des résultats\n",
    "    results = collection.aggregate(pipeline)\n",
    "    print(results)\n",
    "    # Obtenir la date et l'heure actuelles avec le fuseau horaire UTC\n",
    "    # Obtenir le fuseau horaire UTC\n",
    "    utc_timezone = pytz.utc\n",
    "    comments = pd.DataFrame()\n",
    "    for video in results:\n",
    "        # Convertir la date de la vidéo en un objet datetime\n",
    "        video_date = pd.to_datetime(video['date'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "        # Ajouter le fuseau horaire UTC à la date de la vidéo\n",
    "        video_date = video_date.tz_localize(utc_timezone)\n",
    "        # Calculer la différence en jours\n",
    "        elapsed_days = (datetime.now(utc_timezone) - video_date).days\n",
    "        \n",
    "        if elapsed_days >= minElapsedCommentsTime:\n",
    "            print(video['id'])\n",
    "            comments = pd.concat([comments, get_video_commentThreads(youtube, video['id'], maxComments)])\n",
    "            collection.update_many(\n",
    "                {\"id\": video['id']},  # Critère de sélection des documents à mettre à jour\n",
    "                {\"$set\": {\"fetchedComments\": True}}  # Mise à jour du champ fetchedComments à true\n",
    "            )\n",
    "    collection2=db['comments']\n",
    "    collection2.create_index([(\"id\", pymongo.ASCENDING), (\"videoId\", pymongo.ASCENDING)], unique=True)\n",
    "    # Chargement des données depuis le fichier CSV \"comments.csv\"\n",
    "    data_dict = comments.to_dict(orient='records')\n",
    "    for document in data_dict:\n",
    "        try:\n",
    "            collection2.insert_one(document)\n",
    "        except pymongo.errors.DuplicateKeyError as e:\n",
    "        \n",
    "            continue    \n",
    "\n",
    "\n",
    "fetch_topVids_comments_with_bd(minElapsedCommentsTime=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satistics\n",
    "Some stats about the comments and top vids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = pd.read_csv('db/comments.csv')\n",
    "df = pd.read_csv('db/dailyTop200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['topID'] == 1].sort_values(by=['ViewCount', 'likeCount'], ascending=False)\n",
    "\n",
    "vals = df[df['id'] == 'tWYsfOSY9vY'][['title','ViewCount', 'likeCount']].drop_duplicates().values\n",
    "for title,view,like in vals:\n",
    "    print(f'One every {round(view/like)} person droped a like on `{title}`')\n",
    "df\n",
    "# df[df['id'] == '_9u4sYHcR7A'][['topID', 'region', 'fetchedDate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_evolution_plot(dt, id):\n",
    "    data = dt[dt['id'] == id]\n",
    "    data['fetchedDate'] = pd.to_datetime(data['fetchedDate'])\n",
    "    data['fetchedDate'] = data['fetchedDate'].dt.strftime('%d/%m')\n",
    "    \n",
    "    sns.lineplot(data=data[['fetchedDate', 'topID', 'region']], x='fetchedDate', y='topID', hue='region')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Top ID')\n",
    "    plt.title(f\"Top ID `{data['title'].values[0]}` par région au fil du temps\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "top_evolution_plot(df, 'dIlbshbTRlQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = df[df['region'] == 'FR']\n",
    "top1ID = dt[dt['topID'] == 1]['id'].values\n",
    "# print(top1ID)\n",
    "dt = df[df['id'].isin(top1ID)]\n",
    "dt['fetchedDate'] = pd.to_datetime(dt['fetchedDate'])\n",
    "dt['fetchedDate'] = dt['fetchedDate'].dt.strftime('%d/%m')\n",
    "dt.sort_values(by=['fetchedDate'], ascending=False, inplace=True)\n",
    "\n",
    "sns.lineplot(data=dt[['fetchedDate', 'topID', 'title']], x='fetchedDate', y='topID', hue='title', legend=False)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Top ID')\n",
    "plt.title(f\"Top ID 1 en France au fil du temps\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "# dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fetchedDate'].sort_values(ascending=False).unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
