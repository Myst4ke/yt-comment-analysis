{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YT Comments analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from api import API_KEY\n",
    "\n",
    "channel_id = \"UCWeg2Pkate69NFdBeuRFTAw\" #Squeezie channel\n",
    "etoiles = 'UCABf02qOye7XYapcK1M45LQ'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "exemple_video = \"qCKyRhkhqoQ\"\n",
    "otp_recap = 'F7A8OCdmZ90'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    \"\"\" Class Request handling youtube request better \"\"\"\n",
    "    def __init__(self, requestType,part=None, id=None, chart=None, regionCode=None, maxResults=None, pageToken=None, videoId=None):\n",
    "        self.requestType = requestType\n",
    "        self.part = part\n",
    "        self.id = id\n",
    "        self.chart = chart\n",
    "        self.regionCode = regionCode\n",
    "        self.maxResults = maxResults\n",
    "        self.pageToken = pageToken\n",
    "        self.videoId = videoId\n",
    "        \n",
    "    def execute(self):\n",
    "        param = vars(self) # Fetch class attributes\n",
    "        param = {x:y for x,y in list(param.items())[1:] if y} # Delete requestType ([1:]) and None attributes\n",
    "        \n",
    "        request = self.requestType.list(**param)\n",
    "        return request.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def retry_on_exception(max_attempts=5):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempts = 0\n",
    "            while attempts < max_attempts:\n",
    "                try:\n",
    "                    result = func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    attempts += 1\n",
    "                    if attempts == max_attempts:\n",
    "                        return []\n",
    "                        raise  # Relancer l'exception si le nombre maximal de tentatives est atteint\n",
    "                    else:\n",
    "                        print(f\"{attempts}: Une exception s'est produite : {e}\")\n",
    "                else:\n",
    "                    return result  # Retourner le résultat si aucune exception n'est levée\n",
    "                time.sleep(0.5)\n",
    "        return wrapper\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import re      \n",
    "        \n",
    "def iso_toDatetime(iso_date:str):\n",
    "    \"\"\"Converts an ISO 8601 formatted date to a datetime object.\"\"\"\n",
    "    return datetime.strptime(iso_date[:-1], '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "def datetime_toISO(dt_obj:datetime):\n",
    "    \"\"\"Converts a datetime object to an ISO 8601 formatted date.\"\"\"\n",
    "    return dt_obj.isoformat()[:-7]  # remove microseconds\n",
    "\n",
    "def iso_toDelta(iso_duration:str):\n",
    "    \"\"\"Converts an ISO 8601 formatted duration to a timedelta object.\"\"\"\n",
    "    match = re.match(r'PT(\\d+D)*(\\d+H)*(\\d+M)*(\\d+S)', iso_duration)\n",
    "    days, hours, minutes, seconds = [int(x[:-1]) if x else 0 for x in match.groups()]\n",
    "    return timedelta(days=days,hours=hours, minutes=minutes, seconds=seconds)\n",
    "\n",
    "def delta_toISO(delta_obj:timedelta):\n",
    "    \"\"\"Converts a timedelta object to an ISO 8601 formatted duration.\"\"\"\n",
    "    hours = delta_obj.seconds // 3600\n",
    "    minutes = (delta_obj.seconds % 3600) // 60\n",
    "    seconds = delta_obj.seconds % 60\n",
    "    \n",
    "    daysStr = f\"{delta_obj.days}D\" if delta_obj.days != 0 else \"\"\n",
    "    hoursStr = f\"{hours}H\" if hours != 0 else \"\"\n",
    "    minutesStr = f\"{minutes}M\" if minutes != 0 else \"\"\n",
    "    secondsStr = f\"{seconds}S\" if seconds != 0 else \"\"\n",
    "    return f\"PT{daysStr}{hoursStr}{minutesStr}{secondsStr}\"\n",
    "\n",
    "# print(iso_toDelta('PT4D3H20M9S'))\n",
    "# print(delta_toISO(iso_toDelta('PT20M9S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_channel_data(channel_data):\n",
    "    \"\"\" Structure raw channel data \"\"\"\n",
    "    data = {\n",
    "        \"channel_name\": channel_data.get('snippet', {}).get('title'),\n",
    "        \"channel_id\": channel_data.get('id'),\n",
    "        \"country\": channel_data.get('snippet', {}).get('country',\"\"),\n",
    "        \"stats\": channel_data.get('statistics'),\n",
    "        \"topics\": [wikilink.split('/')[-1] for wikilink in channel_data.get('topicDetails', {}).get('topicCategories', [])],\n",
    "    }\n",
    "    del data['stats']['hiddenSubscriberCount']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_data(youtube, channel_id):\n",
    "    \"\"\" Request (by id) for most important channel stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.channels(),\n",
    "        part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    rawData = response.get('items', [])[0]\n",
    "    return format_channel_data(rawData)\n",
    "    \n",
    "\n",
    "get_channel_data(youtube, channel_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def format_video_data(video_data):\n",
    "    \"\"\" Structure raw video data \"\"\"\n",
    "    data = {\n",
    "            \"title\": video_data.get('snippet', {}).get('title'),\n",
    "            \"id\": video_data.get('id'),\n",
    "            \"publishedAt\": video_data.get('snippet', {}).get('publishedAt'),\n",
    "            \"duration\" : video_data.get('contentDetails').get('duration'),\n",
    "            \"ViewCount\" : video_data.get('statistics', {}).get('viewCount'),\n",
    "            \"likeCount\" : video_data.get('statistics', {}).get('likeCount'),\n",
    "            \"commentCount\" : video_data.get('statistics', {}).get('commentCount'),  \n",
    "            \"tags\" : video_data.get('snippet', {}).get('tags')\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "# time = get_video_info(youtube, 'JRBGBjaR9Wg').get(\"publishedAt\")\n",
    "# print(datetime.strptime(time, '%Y-%m-%dT%H:%M:%SZ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(youtube, video_Id):\n",
    "    \"\"\" Request (by id) for most important video stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.videos(),\n",
    "        part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "        id=video_Id,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    rawData = response.get('items', [])[0]\n",
    "    return format_video_data(rawData)\n",
    "\n",
    "get_video_data(youtube, otp_recap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Most_Popular_Video(youtube, region:str):\n",
    "    \"\"\" Request for most populars videos stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.videos(),\n",
    "        part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "        chart=\"mostPopular\",\n",
    "        regionCode=region,\n",
    "        maxResults=100,\n",
    "        pageToken=''\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    pages = [response]\n",
    "    while response.get('nextPageToken'):\n",
    "        request.pageToken = response.get('nextPageToken')\n",
    "        response = request.execute()\n",
    "        pages.append(response)\n",
    "    \n",
    "    top_videos = [format_video_data(videos) for page in pages for videos in page.get('items')]\n",
    "    return top_videos\n",
    "\n",
    "get_Most_Popular_Video(youtube, 'FR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_comment_data(comment):\n",
    "    \"\"\" Structure raw comment data \"\"\"\n",
    "    data = {\n",
    "        \"id\": comment.get('id'),\n",
    "        \"comment\": comment.get('snippet', {}).get('textOriginal'),\n",
    "        # \"viewerRating\": comment.get('snippet', {}).get('viewerRating'),\n",
    "        \"likeCount\": comment.get('snippet', {}).get('likeCount'),\n",
    "        \"publishedAt\": comment.get('snippet', {}).get('publishedAt'),\n",
    "        \"updatedAt\": comment.get('snippet', {}).get('updatedAt')\n",
    "        }\n",
    "    \n",
    "    return data\n",
    "\n",
    "def format_threadedComment_data(comment):\n",
    "    \"\"\" Structure raw threaded comment data \"\"\"\n",
    "    data = {\n",
    "        \"id\": comment.get('id'),\n",
    "        \"topLevelComment\": format_comment_data(comment.get('snippet', {}).get('topLevelComment')),\n",
    "        \"totalReplyCount\": comment.get('snippet', {}).get('totalReplyCount'),\n",
    "        # \"replies\": [format_comment_data(com) for com in comment.get('replies', {}).get('comments', [])]\n",
    "        }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(youtube,comment_id):\n",
    "    \"\"\" Request (by id) for most important comment stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.comments(),\n",
    "        part=\"snippet,id\",\n",
    "        id=comment_id,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    # print(response)\n",
    "    rawData = response.get('items')[0]\n",
    "    return format_comment_data(rawData)\n",
    "\n",
    "get_comment(youtube, 'UgwUQR2JJFJSkihWLhx4AaABAg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "@retry_on_exception(max_attempts=3)\n",
    "def get_video_commentThreads(youtube,video_Id,maxComments):\n",
    "    \"\"\" Request (by id) for all comments of a videos \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.commentThreads(),\n",
    "        part=\"snippet,id,replies\",\n",
    "        videoId=video_Id,\n",
    "        maxResults=100\n",
    "        # pageToken = ''\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    maxComments -= response.get('pageInfo', {}).get('totalResults')\n",
    "    comments = [format_threadedComment_data(comments) for comments in response.get('items',{})]\n",
    "    while response.get('nextPageToken'):\n",
    "        request.pageToken = response.get('nextPageToken')\n",
    "        # time.sleep(0.3)\n",
    "        response = request.execute()\n",
    "        comments += [format_threadedComment_data(comments) for comments in response.get('items',{})]\n",
    "        if (maxComments:= maxComments - response.get('pageInfo', {}).get('totalResults')) <= 0:\n",
    "            break\n",
    "        \n",
    "    print(f\"Fetched {len(comments)} comments !\")\n",
    "    return comments\n",
    "\n",
    "get_video_commentThreads(youtube, otp_recap, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Top Videos\n",
    "The goal is to fetch the top 200 videos everyday and to get their comments a week after publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "REGION = ['FR', 'US']\n",
    "topvids = 'db/topVideos.json'\n",
    "minElapsedTime = 24 # Hours\n",
    "\n",
    "def push_top_vids(filepath):\n",
    "    today = datetime.today()\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    if lastUpdate:= data.get('lastUpdate'):\n",
    "        delta = today - iso_toDatetime(lastUpdate)\n",
    "        if delta.total_seconds() // 3600 < minElapsedTime:\n",
    "            raise Exception(f'The fetch request has be done too soon. Next request available in {24-(delta.total_seconds() // 3600)}h ')\n",
    "        \n",
    "    data['lastUpdate'] = datetime_toISO(today)\n",
    "    # Fetching\n",
    "    for reg in REGION:\n",
    "        if reg not in data.keys():\n",
    "            data[reg] = {}\n",
    "        data[reg][datetime_toISO(today)] = get_Most_Popular_Video(youtube, reg)\n",
    "        \n",
    "    with open(filepath, 'w') as fichier:\n",
    "        json.dump(data, fichier)\n",
    "\n",
    "push_top_vids(topvids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "topvids = 'db/topVideos.json'\n",
    "commentsQueue=\"db/commentQueue.json\"\n",
    "\n",
    "def create_comment_queue(filepath:str) -> None:\n",
    "    with open(filepath, 'r') as f:\n",
    "        data:dict = json.load(f)\n",
    "    with open(commentsQueue, 'r') as f:\n",
    "        queue:dict = json.load(f)   \n",
    "    data.pop('lastUpdate', None)\n",
    "    date_to_fetch = [dates for dates in list(list(data.values())[0].keys()) if dates not in queue.keys()]\n",
    "    \n",
    "    if date_to_fetch: # is empty\n",
    "        for date in date_to_fetch:  \n",
    "            comment_list = []\n",
    "            for region in data.keys():\n",
    "                # print(f\"{region}: {date}. Size {len(data.get(region,{}).get(date,[]))}\")\n",
    "                for video in data.get(region,{}).get(date,[]):\n",
    "                    comment_list += [\n",
    "                        {'region': region, \n",
    "                        'dateEntry': date, \n",
    "                        'id': video.get('id'),\n",
    "                        'publishedAt': video.get('publishedAt')\n",
    "                        }\n",
    "                    ]\n",
    "            queue[date] = comment_list\n",
    "        \n",
    "        with open(commentsQueue, 'w') as fichier:\n",
    "            json.dump(queue, fichier)    \n",
    "    else:\n",
    "        print(\"already in !\")\n",
    "    \n",
    "create_comment_queue(topvids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import json\n",
    "commentsQueue=\"db/commentQueue.json\"\n",
    "commentList = \"db/commentList.json\"\n",
    "minElapsedComments = 17 # days\n",
    "\n",
    "def fetch_topVids_comments(filepath):\n",
    "    today = datetime.today()\n",
    "    with open(filepath, 'r') as f:\n",
    "        queue:dict[list] = json.load(f)\n",
    "    with open(commentList, 'r') as f:\n",
    "        comments:dict[list] = json.load(f)\n",
    "    \n",
    "    data={}\n",
    "    remove_indices:dict[list] = {}\n",
    "    for date,vids in queue.items():\n",
    "        for i,video in enumerate(vids):\n",
    "            if (today - iso_toDatetime(video.get('publishedAt'))).days >= minElapsedComments: \n",
    "                start = time.time()\n",
    "                data[video.get('id')] = get_video_commentThreads(youtube, video.get('id'), 1000) # crashes a lot\n",
    "                print(f\"{i} in {time.time()-start}s\",end='\\n\\n')\n",
    "                if date not in remove_indices.keys():\n",
    "                     remove_indices[date] = []\n",
    "                remove_indices[date].append(i)\n",
    "            else:print(f\"{i} not yet\",end='\\n\\n')\n",
    "        \n",
    "    queue = {date:[video for i, video in enumerate(videos) if i not in remove_indices[date]] for date,videos in queue.items()}  \n",
    "     \n",
    "    with open(filepath, 'w') as fichier:\n",
    "            json.dump(queue, fichier)  \n",
    "               \n",
    "    with open(commentList, 'w') as fichier:\n",
    "            json.dump({**comments, **data}, fichier) # merging data\n",
    "            \n",
    "            \n",
    "fetch_topVids_comments(commentsQueue)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
