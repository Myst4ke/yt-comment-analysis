{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YT Comments analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from datetime import datetime, timedelta, time\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from api import API_KEY\n",
    "\n",
    "channel_id = \"UCWeg2Pkate69NFdBeuRFTAw\" #Squeezie channel\n",
    "etoiles = 'UCABf02qOye7XYapcK1M45LQ'\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "exemple_video = \"qCKyRhkhqoQ\"\n",
    "otp_recap = 'F7A8OCdmZ90'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class request\n",
    "Class to handle youtube request since youtube api doesn't provide a request object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    \"\"\" Class Request handling youtube request as an object \"\"\"\n",
    "    def __init__(self, requestType,part=None, id=None, chart=None, regionCode=None, maxResults=None, pageToken=None, videoId=None):\n",
    "        self.requestType = requestType\n",
    "        self.part = part\n",
    "        self.id = id\n",
    "        self.chart = chart\n",
    "        self.regionCode = regionCode\n",
    "        self.maxResults = maxResults\n",
    "        self.pageToken = pageToken\n",
    "        self.videoId = videoId\n",
    "        \n",
    "    def execute(self):\n",
    "        param = vars(self) # Fetch class attributes\n",
    "        param = {x:y for x,y in list(param.items())[1:] if y} # Delete requestType ([1:]) and None attributes\n",
    "        \n",
    "        request = self.requestType.list(**param)\n",
    "        return request.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorator\n",
    "Decorator to retry when youtube request fails (mostly due to timeout erros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_on_exception(max_attempts=5):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempts = 0\n",
    "            while attempts < max_attempts:\n",
    "                try:\n",
    "                    result = func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    attempts += 1\n",
    "                    if attempts == max_attempts:\n",
    "                        return []\n",
    "                        raise  # Relancer l'exception si le nombre maximal de tentatives est atteint\n",
    "                    else:\n",
    "                        print(f\"{attempts}: Une exception s'est produite : {e}\")\n",
    "                else:\n",
    "                    return result  # Retourner le résultat si aucune exception n'est levée\n",
    "                # time.sleep(0.5)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime convertions\n",
    "Functions to convert iso formated date found in youtube api responses in datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_toDatetime(iso_date:str):\n",
    "    \"\"\"Converts an ISO 8601 formatted date to a datetime object.\"\"\"\n",
    "    return datetime.strptime(iso_date[:-1], '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "def datetime_toISO(dt_obj:datetime):\n",
    "    \"\"\"Converts a datetime object to an ISO 8601 formatted date.\"\"\"\n",
    "    return dt_obj.isoformat()[:-7]  # remove microseconds\n",
    "\n",
    "def iso_toDelta(iso_duration:str):\n",
    "    \"\"\"Converts an ISO 8601 formatted duration to a timedelta object.\"\"\"\n",
    "    match = re.match(r'PT(\\d+D)*(\\d+H)*(\\d+M)*(\\d+S)', iso_duration)\n",
    "    days, hours, minutes, seconds = [int(x[:-1]) if x else 0 for x in match.groups()]\n",
    "    return timedelta(days=days,hours=hours, minutes=minutes, seconds=seconds)\n",
    "\n",
    "def delta_toISO(delta_obj:timedelta):\n",
    "    \"\"\"Converts a timedelta object to an ISO 8601 formatted duration.\"\"\"\n",
    "    hours = delta_obj.seconds // 3600\n",
    "    minutes = (delta_obj.seconds % 3600) // 60\n",
    "    seconds = delta_obj.seconds % 60\n",
    "    \n",
    "    daysStr = f\"{delta_obj.days}D\" if delta_obj.days != 0 else \"\"\n",
    "    hoursStr = f\"{hours}H\" if hours != 0 else \"\"\n",
    "    minutesStr = f\"{minutes}M\" if minutes != 0 else \"\"\n",
    "    secondsStr = f\"{seconds}S\" if seconds != 0 else \"\"\n",
    "    return f\"PT{daysStr}{hoursStr}{minutesStr}{secondsStr}\"\n",
    "\n",
    "# print(iso_toDelta('PT4D3H20M9S'))\n",
    "# print(delta_toISO(iso_toDelta('PT20M9S')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fectching functions\n",
    "Functions to fetch channels, comments ant top vids infos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_channel_data(channel_data: dict) -> dict[str|dict]:\n",
    "    \"\"\" Structure raw channel data \"\"\"\n",
    "    data = {\n",
    "        \"channel_name\": channel_data.get('snippet', {}).get('title'),\n",
    "        \"channel_id\": channel_data.get('id'),\n",
    "        \"country\": channel_data.get('snippet', {}).get('country',\"\"),\n",
    "        \"stats\": channel_data.get('statistics'),\n",
    "        \"topics\": [wikilink.split('/')[-1] for wikilink in channel_data.get('topicDetails', {}).get('topicCategories', [])],\n",
    "    }\n",
    "    del data['stats']['hiddenSubscriberCount']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_channel_dataDF(channel_data: dict) -> pd.DataFrame:\n",
    "    \"\"\" Structure raw channel data \"\"\"\n",
    "    data = {\n",
    "        \"channel_name\": [channel_data.get('snippet', {}).get('title')],\n",
    "        \"channel_id\": [channel_data.get('id')],\n",
    "        \"country\": [channel_data.get('snippet', {}).get('country',\"\")],\n",
    "        **{k:[int(v)] for k,v in channel_data.get('statistics', {}).items() if k != \"hiddenSubscriberCount\"},\n",
    "        \"topics\": [[wikilink.split('/')[-1] for wikilink in channel_data.get('topicDetails', {}).get('topicCategories', [])]],\n",
    "    }\n",
    "    # del data['stats']['hiddenSubscriberCount']\n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_data(youtube, channel_id:str) -> dict[str|dict]:\n",
    "    \"\"\" Request (by id) for most important channel stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.channels(),\n",
    "        part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    rawData = response.get('items', [])[0]\n",
    "    return format_channel_dataDF(rawData)\n",
    "    \n",
    "\n",
    "get_channel_data(youtube, etoiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_video_data(video_data: dict) -> dict[str|dict]:\n",
    "    \"\"\" Structure raw video data \"\"\"\n",
    "    data = {\n",
    "            \"title\": video_data.get('snippet', {}).get('title'),\n",
    "            \"id\": video_data.get('id'),\n",
    "            \"publishedAt\": video_data.get('snippet', {}).get('publishedAt'),\n",
    "            \"duration\" : video_data.get('contentDetails').get('duration'),\n",
    "            \"ViewCount\" : video_data.get('statistics', {}).get('viewCount'),\n",
    "            \"likeCount\" : video_data.get('statistics', {}).get('likeCount'),\n",
    "            \"commentCount\" : video_data.get('statistics', {}).get('commentCount'),  \n",
    "            \"tags\" : video_data.get('snippet', {}).get('tags')\n",
    "    }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_video_dataDF(video_data: dict) -> pd.DataFrame:\n",
    "    \"\"\" Structure raw video data \"\"\"\n",
    "    data = {\n",
    "            \"title\": [video_data.get('snippet', {}).get('title')],\n",
    "            \"id\": [video_data.get('id')],\n",
    "            \"publishedAt\": [video_data.get('snippet', {}).get('publishedAt')],\n",
    "            \"duration\" : [video_data.get('contentDetails').get('duration')],\n",
    "            \"ViewCount\" : [int(video_data.get('statistics', {}).get('viewCount', 0))],\n",
    "            \"likeCount\" : [int(video_data.get('statistics', {}).get('likeCount', 0))],\n",
    "            \"commentCount\" : [int(video_data.get('statistics', {}).get('commentCount', 0))],  \n",
    "            \"tags\" : [video_data.get('snippet', {}).get('tags')]\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(youtube, video_Id:str) -> dict[str|dict]:\n",
    "    \"\"\" Request (by id) for most important video stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.videos(),\n",
    "        part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "        id=video_Id,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    rawData = response.get('items', [])[0]\n",
    "    return format_video_dataDF(rawData)\n",
    "\n",
    "get_video_data(youtube, exemple_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Most_Popular_Video(youtube, region:str) -> pd.DataFrame:\n",
    "    \"\"\" Request for most populars videos stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.videos(),\n",
    "        part=\"snippet,contentDetails,statistics,topicDetails\",\n",
    "        chart=\"mostPopular\",\n",
    "        regionCode=region,\n",
    "        maxResults=100,\n",
    "        pageToken=''\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    pages = [response]\n",
    "    while response.get('nextPageToken'):\n",
    "        request.pageToken = response.get('nextPageToken')\n",
    "        response = request.execute()\n",
    "        pages.append(response)\n",
    "    \n",
    "    \n",
    "    # top = [format_video_data(videos) for page in pages for videos in page.get('items')]\n",
    "    # print(sorted(top, key=lambda d: d['ViewCount']))\n",
    "    top_videos = pd.concat([format_video_dataDF(videos) for page in pages for videos in page.get('items')]).reset_index(drop=True)\n",
    "    top_videos['fetchedDate'] = datetime.today()\n",
    "    top_videos['topID'] = top_videos.index + 1\n",
    "    top_videos['region'] = region\n",
    "    return top_videos\n",
    "# pd.DataFrame.sort_values\n",
    "\n",
    "df:pd.DataFrame = get_Most_Popular_Video(youtube, 'FR')\n",
    "df.sort_values(by=['fetchedDate'], ascending=False, inplace=True, kind='quicksort', ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_comment_data(comment:dict) -> dict[str|dict]:\n",
    "    \"\"\" Structure raw comment data \"\"\"\n",
    "    data = {\n",
    "        \"id\": comment.get('id'),\n",
    "        \"comment\": comment.get('snippet', {}).get('textOriginal'),\n",
    "        # \"viewerRating\": comment.get('snippet', {}).get('viewerRating'),\n",
    "        \"likeCount\": comment.get('snippet', {}).get('likeCount'),\n",
    "        \"publishedAt\": comment.get('snippet', {}).get('publishedAt'),\n",
    "        \"updatedAt\": comment.get('snippet', {}).get('updatedAt')\n",
    "        }\n",
    "    \n",
    "    return data\n",
    "\n",
    "def format_threadedComment_data(comment:dict) -> dict[str|dict]:\n",
    "    \"\"\" Structure raw threaded comment data \"\"\"\n",
    "    data = {\n",
    "        **format_comment_data(comment.get('snippet', {}).get('topLevelComment')),\n",
    "        \"totalReplyCount\": comment.get('snippet', {}).get('totalReplyCount'),\n",
    "        # \"replies\": [format_comment_data(com) for com in comment.get('replies', {}).get('comments', [])]\n",
    "        }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_comment_dataDF(comment:dict) -> pd.DataFrame:\n",
    "    \"\"\" Structure raw comment data \"\"\"\n",
    "    data = {\n",
    "        \"id\": [comment.get('id')],\n",
    "        \"comment\": [comment.get('snippet', {}).get('textOriginal')],\n",
    "        # \"viewerRating\": comment.get('snippet', {}).get('viewerRating'),\n",
    "        \"likeCount\": [int(comment.get('snippet', {}).get('likeCount'))],\n",
    "        \"publishedAt\": [comment.get('snippet', {}).get('publishedAt')],\n",
    "        \"updatedAt\": [comment.get('snippet', {}).get('updatedAt')]\n",
    "        }\n",
    "    \n",
    "    return data\n",
    "\n",
    "def format_threadedComment_dataDF(comment:dict) -> pd.DataFrame:\n",
    "    \"\"\" Structure raw threaded comment data \"\"\"\n",
    "    data = {\n",
    "        **format_comment_data(comment.get('snippet', {}).get('topLevelComment')),\n",
    "        \"totalReplyCount\": [int(comment.get('snippet', {}).get('totalReplyCount'))],\n",
    "        # \"replies\": [format_comment_data(com) for com in comment.get('replies', {}).get('comments', [])]\n",
    "        }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_commentPageDF(page:list[dict]) -> pd.DataFrame:\n",
    "    data = {}\n",
    "    for comment in page:\n",
    "        topLevelComment = comment.get('snippet', {}).get('topLevelComment')\n",
    "        data.setdefault(\"id\", []).append(topLevelComment.get('id')) \n",
    "        data.setdefault(\"comment\", []).append(topLevelComment.get('snippet', {}).get('textOriginal'))\n",
    "        data.setdefault(\"likeCount\", []).append(int(topLevelComment.get('snippet', {}).get('likeCount')))\n",
    "        data.setdefault(\"publishedAt\", []).append(topLevelComment.get('snippet', {}).get('publishedAt'))\n",
    "        data.setdefault(\"updatedAt\", []).append(topLevelComment.get('snippet', {}).get('updatedAt'))\n",
    "        data.setdefault(\"totalReplyCount\", []).append(int(comment.get('snippet', {}).get('totalReplyCount')))\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(youtube,comment_id:str) -> dict[str|dict]:\n",
    "    \"\"\" Request (by id) for most important comment stats \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.comments(),\n",
    "        part=\"snippet,id\",\n",
    "        id=comment_id,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    rawData = response.get('items')[0]\n",
    "    return format_comment_data(rawData)\n",
    "\n",
    "get_comment(youtube, 'UgwUQR2JJFJSkihWLhx4AaABAg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @retry_on_exception(max_attempts=3)\n",
    "def get_video_commentThreads(youtube, videoID:str, maxComments:int) -> dict[str|list]:\n",
    "    \"\"\" Request (by id) for all comments of a videos \"\"\"\n",
    "    request = Request(\n",
    "        requestType=youtube.commentThreads(),\n",
    "        part=\"snippet,id,replies\",\n",
    "        videoId=videoID,\n",
    "        maxResults=100\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    maxComments -= response.get('pageInfo', {}).get('totalResults')\n",
    "    comments = format_commentPageDF(response.get('items',{}))\n",
    "    while response.get('nextPageToken'):\n",
    "        request.pageToken = response.get('nextPageToken')\n",
    "        response = request.execute()\n",
    "        comments = pd.concat([comments, format_commentPageDF(response.get('items',{}))])\n",
    "        if (maxComments:= maxComments - response.get('pageInfo', {}).get('totalResults')) <= 0:\n",
    "            break\n",
    "        \n",
    "    comments['videoID'] = videoID\n",
    "    comments['fetchedDate'] = datetime.today()\n",
    "    return comments\n",
    "\n",
    "commentsDF = get_video_commentThreads(youtube, 'FkXhKu80CWU', 1000).reset_index().sort_values(by=['likeCount', 'totalReplyCount'], ascending=False, ignore_index=True)\n",
    "commentsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Top Videos\n",
    "The goal is to fetch the top 200 videos everyday and to get their comments a week after publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_toDF(topvidsFile):\n",
    "    \"\"\" transfrom topVideos.json to dataframe then to .csv\"\"\"\n",
    "    with open(topvidsFile, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    data.pop('lastUpdate')\n",
    "    videosList = {}\n",
    "    for region,dateEntries in data.items():\n",
    "        # print(region, type(dateEntries))\n",
    "        for date, videos in dateEntries.items():\n",
    "            for id, video in enumerate(videos):\n",
    "                videosList.setdefault('title', []).append(video.get('title'))\n",
    "                videosList.setdefault('id', []).append(video.get('id'))\n",
    "                videosList.setdefault('publishedAt', []).append(video.get('publishedAt'))\n",
    "                videosList.setdefault('duration', []).append(video.get('duration'))\n",
    "                videosList.setdefault('ViewCount', []).append(video.get('ViewCount'))\n",
    "                videosList.setdefault('likeCount', []).append(video.get('likeCount'))\n",
    "                videosList.setdefault('commentCount', []).append(video.get('commentCount'))\n",
    "                videosList.setdefault('tags', []).append(video.get('tags'))\n",
    "                videosList.setdefault('fetchedDate', []).append(date) \n",
    "                videosList.setdefault('topID', []).append(id+1)\n",
    "                videosList.setdefault('region', []).append(region)\n",
    "    \n",
    "    df = pd.DataFrame(videosList)\n",
    "    df.to_csv('db/top.csv', index=False)\n",
    "    # fetchedDate topID region\n",
    "json_toDF('db/topVideos.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The fetch request has be done too soon. Next request available in 24.0h. Last update done : 2024-03-02 18:04:56.339221",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df   \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# with open(topvidsFile, 'w') as fichier:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#     json.dump(data, fichier)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mpush_top_vidsDF\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdb/dailyTop200.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 14\u001b[0m, in \u001b[0;36mpush_top_vidsDF\u001b[0;34m(topvidsFile, regions, minElapsedTime)\u001b[0m\n\u001b[1;32m     12\u001b[0m     delta \u001b[38;5;241m=\u001b[39m today \u001b[38;5;241m-\u001b[39m lastUpdate\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m delta\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600\u001b[39m \u001b[38;5;241m<\u001b[39m minElapsedTime:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe fetch request has be done too soon. Next request available in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m24\u001b[39m\u001b[38;5;241m-\u001b[39m(delta\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m3600\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mh. Last update done : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlastUpdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Fetching\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reg \u001b[38;5;129;01min\u001b[39;00m regions:\n",
      "\u001b[0;31mException\u001b[0m: The fetch request has be done too soon. Next request available in 24.0h. Last update done : 2024-03-02 18:04:56.339221"
     ]
    }
   ],
   "source": [
    "def push_top_vidsDF(topvidsFile:str, regions:list[str], minElapsedTime:int)-> None:\n",
    "    \"\"\" Fetch top 200 vids per region (per <minElapsedTime>) and push in json <topvidsFile> \"\"\"\n",
    "    today = datetime.today()\n",
    "    try:\n",
    "        df = pd.read_csv(topvidsFile)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    if 'fetchedDate' in df:\n",
    "        df['fetchedDate'] = pd.to_datetime(df['fetchedDate'], format='ISO8601')\n",
    "        lastUpdate = df.sort_values(by=['fetchedDate'], ascending=False, ignore_index=True).loc[0]['fetchedDate']\n",
    "        delta = today - lastUpdate\n",
    "        if delta.total_seconds() // 3600 < minElapsedTime:\n",
    "            raise Exception(f'The fetch request has be done too soon. Next request available in {24-(delta.total_seconds() // 3600)}h. Last update done : {lastUpdate}')\n",
    "\n",
    "    # Fetching\n",
    "    for reg in regions:\n",
    "        df = pd.concat([df, get_Most_Popular_Video(youtube, reg)])\n",
    "        \n",
    "    df.to_csv('db/dailyTop200.csv', index=False) \n",
    "\n",
    "push_top_vidsDF('db/dailyTop200.csv', ['FR', 'US'], 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_top_vids(topvidsFile:str, regions:list[str], minElapsedTime:int)-> None:\n",
    "    \"\"\" Fetch top 200 vids per region (per <minElapsedTime>) and push in json <topvidsFile> \"\"\"\n",
    "    today = datetime.today()\n",
    "    with open(topvidsFile, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    if lastUpdate:= data.get('lastUpdate'):\n",
    "        delta = today - iso_toDatetime(lastUpdate)\n",
    "        if delta.total_seconds() // 3600 < minElapsedTime:\n",
    "            raise Exception(f'The fetch request has be done too soon. Next request available in {24-(delta.total_seconds() // 3600)}h ')\n",
    "        \n",
    "    data['lastUpdate'] = datetime_toISO(today)\n",
    "    # Fetching\n",
    "    for reg in regions:\n",
    "        if reg not in data.keys():\n",
    "            data[reg] = {}\n",
    "        data[reg][datetime_toISO(today)] = get_Most_Popular_Video(youtube, reg)\n",
    "        \n",
    "    with open(topvidsFile, 'w') as fichier:\n",
    "        json.dump(data, fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comment_queue(topvidsFile:str, commentsQueue:str) -> None:\n",
    "    \"\"\" Create a json file <commentsQueue> that stores video IDs which comments have not been fetched \"\"\"\n",
    "    with open(topvidsFile, 'r') as f:\n",
    "        data:dict = json.load(f)\n",
    "    with open(commentsQueue, 'r') as f:\n",
    "        queue:dict = json.load(f)   \n",
    "        \n",
    "    data.pop('lastUpdate', None)\n",
    "    date_to_fetch = [dates for dates in list(list(data.values())[0].keys()) if dates not in queue.keys()]\n",
    "    \n",
    "    if date_to_fetch: # is empty\n",
    "        for date in date_to_fetch:  \n",
    "            comment_list = []\n",
    "            for region in data.keys():\n",
    "                # print(f\"{region}: {date}. Size {len(data.get(region,{}).get(date,[]))}\")\n",
    "                for video in data.get(region,{}).get(date,[]):\n",
    "                    comment_list += [\n",
    "                        {'region': region, \n",
    "                        'dateEntry': date, \n",
    "                        'id': video.get('id'),\n",
    "                        'publishedAt': video.get('publishedAt')\n",
    "                        }\n",
    "                    ]\n",
    "            queue[date] = comment_list\n",
    "        \n",
    "        with open(commentsQueue, 'w') as fichier:\n",
    "            json.dump(queue, fichier)    \n",
    "    else:\n",
    "        print(\"already in !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_topVids_comments(commentsQueue:str, commentList:str, minElapsedCommentsTime:int) -> None:\n",
    "    \"\"\" Fetches vids comments from <commentsQueue> after minElapsedComments and output them in <commentList>\"\"\"\n",
    "    today = datetime.today()\n",
    "    with open(commentsQueue, 'r') as f:\n",
    "        queue:dict[list] = json.load(f)\n",
    "    with open(commentList, 'r') as f:\n",
    "        comments:dict[list] = json.load(f)\n",
    "    \n",
    "    data={}\n",
    "    remove_indices:dict[list] = {}\n",
    "    for date,vids in queue.items():\n",
    "        for i,video in enumerate(vids):\n",
    "            if (today - iso_toDatetime(video.get('publishedAt'))).days >= minElapsedCommentsTime: \n",
    "                start = time.time()\n",
    "                data[video.get('id')] = get_video_commentThreads(youtube, video.get('id'), 1000) # crashes a lot\n",
    "                print(f\"{i} in {time.time()-start}s\",end='\\n\\n')\n",
    "                if date not in remove_indices.keys():\n",
    "                     remove_indices[date] = []\n",
    "                remove_indices[date].append(i)\n",
    "            else:print(f\"{i} not yet\",end='\\n\\n')\n",
    "        \n",
    "    queue = {date:[video for i, video in enumerate(videos) if i not in remove_indices[date]] for date,videos in queue.items()}  \n",
    "     \n",
    "    with open(commentsQueue, 'w') as fichier:\n",
    "            json.dump(queue, fichier)  \n",
    "               \n",
    "    with open(commentList, 'w') as fichier:\n",
    "            json.dump({**comments, **data}, fichier) # merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = ['FR', 'US']\n",
    "topvids = 'db/topVideos.json'\n",
    "commentsQueue=\"db/commentQueue.json\"\n",
    "commentList = \"db/commentList.json\"\n",
    "minElapsedTime = 24 # Hours\n",
    "minElapsedCommentsTime = 17 # days\n",
    "\n",
    "push_top_vids(topvids, REGION, minElapsedTime)\n",
    "    \n",
    "# create_comment_queue(topvids, commentsQueue)\n",
    "            \n",
    "# fetch_topVids_comments(commentsQueue, commentList, minElapsedCommentsTime)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
